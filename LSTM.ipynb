{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADL_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ONPr8fww6Qr9yZUXtdur3I9GoI_0Fxv_",
      "authorship_tag": "ABX9TyOCKR6Ujt9RnVyK35sDkjlp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/NLP_Project/blob/main/ADL_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwcXAhDiPnv1"
      },
      "source": [
        "#replace all emojis with phrases\n",
        "import re\n",
        "def replace_emojis(text):\n",
        "  happyface = \":\\)\"\n",
        "  text = re.sub(r\"http.*\",\"url\",text)\n",
        "  text = re.sub(happyface,\"happyface\",text)\n",
        "  happyface = \":-\\)\"\n",
        "  text = re.sub(happyface,\"happyface\",text)\n",
        "  happyface = \":-\\]\"\n",
        "  text = re.sub(happyface,\"happyface\",text)\n",
        "  happyface = \":\\]\"\n",
        "  text = re.sub(happyface,\"happyface\",text)\n",
        "  happyface = \"=\\)\"\n",
        "  text = re.sub(happyface,\"happyface\",text)\n",
        "  happyface = \":\\'-\\)\"\n",
        "  text = re.sub(happyface,\"happyface\",text)\n",
        "  happyface = \":\\'\\)\"\n",
        "  text = re.sub(happyface,\"happyface\",text)\n",
        "  sadface = \":\\(\"\n",
        "  text = re.sub(sadface,\"sadface\",text)\n",
        "  sadface = \":-\\(\"\n",
        "  text = re.sub(sadface,\"sadface\",text)\n",
        "  sadface = \":\\[\"\n",
        "  text = re.sub(sadface,\"sadface\",text)\n",
        "  sadface = \":-\\[\"\n",
        "  text = re.sub(sadface,\"sadface\",text)\n",
        "  sadface = \":\\'-\\(\"\n",
        "  text = re.sub(sadface,\"sadface\",text)\n",
        "  sadface = \":\\'\\(\"\n",
        "  text = re.sub(sadface,\"sadface\",text)\n",
        "  sadface = \"\\(:\"\n",
        "  text = re.sub(sadface,\"sadface\",text)\n",
        "  # text = re.sub(\":|\",\"neutralface\",text)\n",
        "  text = re.sub(\":P\",\"lolface\",text)\n",
        "  text = re.sub(\":p\",\"lolface\",text)\n",
        "  text = re.sub(\":o\",\"surprise\",text)\n",
        "  text = re.sub(\":O\",\"surprise\",text)\n",
        "  #print(text[0:1000])\n",
        "  return text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJcfgy51Pti9"
      },
      "source": [
        "def split_into_words(text):\n",
        "  sentences = text.splitlines()\n",
        "  words_sentences = []\n",
        "  for sent in sentences:\n",
        "    #words = sent.split(\"\\t\")\n",
        "    words = sent.split()\n",
        "    words_sentences.append(words)\n",
        "  #print(words_sentences[0:10])\n",
        "  return words_sentences"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mxg00CIPzhU"
      },
      "source": [
        "def seperate_classes_ids_sentences(word_sentences):\n",
        "  sentence_ids = []\n",
        "  sentence_classes = []\n",
        "  sentences = []\n",
        "  for sent in word_sentences:\n",
        "    if len(sent)==0:\n",
        "      continue\n",
        "    if sent[0].isdigit():\n",
        "      sentence_ids.append(int(sent[0]))\n",
        "    else:\n",
        "      continue\n",
        "    sentence_classes.append(sent[1])\n",
        "    words = sent[2:]\n",
        "    #print(words)\n",
        "    sentences.append(words)\n",
        "  print(sentence_ids)\n",
        "  print(sentence_classes)\n",
        "  print(sentences[0:5])\n",
        "  return sentences,sentence_classes,sentence_ids"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bacxxQnP3YK"
      },
      "source": [
        "import re\n",
        "def remove_punctuations(sentences):\n",
        "  def remove_punctuations_word(input_string):\n",
        "    input_string = re.sub(r'\\\\u002c','',input_string)\n",
        "    input_string = re.sub(r'@.*','',input_string)\n",
        "    input_string = re.sub(r'#.*',' ',input_string)\n",
        "    input_string = re.sub(r'[0-9]+', '', input_string)\n",
        "    res = re.sub(r'[^\\w\\s]', '', input_string)\n",
        "    return res\n",
        "  for sent in sentences:\n",
        "    i = 0\n",
        "    while i<len(sent):\n",
        "      word = remove_punctuations_word(sent[i])\n",
        "      sent[i] = word\n",
        "      if len(sent[i])==0:\n",
        "        sent.pop(i)\n",
        "        i -= 1 \n",
        "      i += 1\n",
        "  print(sentences[0:10])\n",
        "  return sentences"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKbmpP8VwZ1c"
      },
      "source": [
        "stopwords = [\"ourselves\", \"hers\", \"between\", \"yourself\",\"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"having\",\n",
        "             \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"itself\", \"other\",\n",
        "             \"off\", \"is\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\",\n",
        "             \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\",\n",
        "             \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"when\", \"at\", \"any\", \"before\", \"them\",\n",
        "             \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"what\", \"over\", \"why\",\n",
        "             \"so\", \"can\", \"did\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\",  \"where\", \"only\", \"myself\", \n",
        "             \"which\", \"those\", \"i\", \"after\", \"whom\", \"to\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\",\n",
        "             \"how\", \"further\", \"was\", \"here\", \"than\",\"one\",\"time\"]\n",
        "def remove_stopwords(words_sentences):\n",
        "  filtered_words = []\n",
        "  for sentence in words_sentences:\n",
        "    filtered_sentence = []\n",
        "    for word in sentence:\n",
        "      if word not in stopwords:\n",
        "        filtered_sentence.append(word)\n",
        "    filtered_words.append(filtered_sentence)\n",
        "  return filtered_words"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2tORRN5QMof"
      },
      "source": [
        "def remove_recurring_characters(sentences):\n",
        "  def remove_recurring_characters_in_word(word):\n",
        "    i = 1\n",
        "    rec_count = 0\n",
        "    while i < len(word):\n",
        "      if word[i] == word[i-1]:\n",
        "        rec_count += 1\n",
        "      else:\n",
        "        rec_count =0\n",
        "      if rec_count>=2:\n",
        "        word = word[:i]+word[i+1:]\n",
        "        i -= 1\n",
        "      i += 1\n",
        "    return word\n",
        "  for sent in sentences:\n",
        "    for i in range(len(sent)):\n",
        "      sent[i] = remove_recurring_characters_in_word(sent[i]).lower()\n",
        "  print(sentences[0:10])\n",
        "  return sentences"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6xNaySDUIMh"
      },
      "source": [
        "def get_train_sentence_vectors(sentences,model):\n",
        "  words_to_integer_dict = dict()\n",
        "  integer_to_word_dict = dict()\n",
        "  print(\"No. of sentences = \",len(sentences))\n",
        "  sentence_vectors = []\n",
        "  max_sentence_length = 0\n",
        "  word_count = 1\n",
        "  for sent in sentences:\n",
        "    vec = []\n",
        "    sent_length = 0\n",
        "    for word in sent:\n",
        "      sent_length += 1\n",
        "      if words_to_integer_dict.get(word) == None:\n",
        "        if word in model:\n",
        "          words_to_integer_dict[word] = word_count\n",
        "          vec.append(word_count)\n",
        "          word_count += 1\n",
        "        else:\n",
        "          vec.append(0)\n",
        "      else:\n",
        "        vec.append(words_to_integer_dict[word])\n",
        "    max_sentence_length = max(sent_length,max_sentence_length)\n",
        "    sentence_vectors.append(vec)\n",
        "  return sentence_vectors,word_count,max_sentence_length,words_to_integer_dict"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QkbhQ3wjl8u"
      },
      "source": [
        "def get_embeds_matrix(embeds_list):\n",
        "  embeds_matrix = []\n",
        "  for i in range(len(embeds_list)):\n",
        "    embeds_list[i] = str(embeds_list[i][1:len(embeds_list[i])-1])\n",
        "    array = embeds_list[i].split()\n",
        "    for j in range(0,len(array)):\n",
        "      array[j] = float(array[j])\n",
        "    embeds_matrix.append(array)\n",
        "  \n",
        "  return np.asarray(embeds_matrix)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agqRhZV7QoEE"
      },
      "source": [
        "#initializing the embedding matrix for embedding layer in the model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "def get_initial_embeddings(model_ft,word_count,words_to_integer_dict):\n",
        "  embeds_file = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/distant_training_embeddings.csv\")\n",
        "  count_in_model = 0\n",
        "  embedding_matrix = [[0]*50]*(word_count+2)  #one for not in model one for not in train data\n",
        "  words_list = embeds_file['words'].tolist()\n",
        "  embeds_list = embeds_file['embeds'].tolist()\n",
        "  embeds_matrix = get_embeds_matrix(embeds_list)\n",
        "  for word in words_to_integer_dict:\n",
        "    if word in words_list:\n",
        "      embedding_matrix[words_to_integer_dict[word]] = np.asarray(embeds_matrix[words_list.index(word)])\n",
        "      count_in_model += 1\n",
        "    elif word in model_ft:\n",
        "      count_in_model += 1\n",
        "      embedding_matrix[words_to_integer_dict[word]] = model[word]\n",
        "  embedding_matrix = np.asarray(embedding_matrix,dtype=float)\n",
        "  print(\"No. of embeddings in model = \",count_in_model)\n",
        "  return embedding_matrix"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4NzHAPLBLZv"
      },
      "source": [
        "def get_test_sentence_vectors(sentences,word_count):\n",
        "  sentence_vectors = []\n",
        "  for sent in sentences:\n",
        "    vec = []\n",
        "    for word in sent:\n",
        "      if words_to_integer_dict.get(word) != None:\n",
        "        vec.append(words_to_integer_dict[word])\n",
        "      else:\n",
        "        vec.append(word_count+1)\n",
        "    sentence_vectors.append(vec)\n",
        "  return sentence_vectors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1haR5W4Mp150"
      },
      "source": [
        "def pad_input_vectors(sentence_vectors,max_sentence_length):\n",
        "  for i in range(len(sentence_vectors)):\n",
        "    sentence_vectors[i] = sentence_vectors[i][0:max_sentence_length]\n",
        "    for j in range(max_sentence_length-len(sentence_vectors[i])):\n",
        "      sentence_vectors[i].append(0)\n",
        "  #print(sentence_vectors[0:2])\n",
        "  data_x = np.asarray(sentence_vectors)\n",
        "  return data_x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnAlreugXQpl"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "def get_input_class_labels(sentence_classes):\n",
        "  le.fit(sentence_classes)\n",
        "  data_y = le.transform(sentence_classes)\n",
        "  data_y = np.asarray(data_y)\n",
        "  return data_y\n",
        "def get_test_class_labels(sentence_classes):\n",
        "  test_y = le.transform(sentence_classes)\n",
        "  return np.asarray(test_y)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilEfjc9kXUCe"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def split_train_data(data_x,data_y):\n",
        "  train_x,dev_x,train_y,dev_y = train_test_split(data_x,data_y,test_size=0.25,random_state=20,stratify=data_y)\n",
        "  return train_x,dev_x,train_y,dev_y"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1clual0hWxyH"
      },
      "source": [
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Softmax, Bidirectional, Flatten, Average\n",
        "from keras.models import Sequential,Model\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_weights(train_y):\n",
        "  count = np.zeros(3,dtype = float)\n",
        "  for i in range(len(train_y)):\n",
        "    count[train_y[i]] += 1\n",
        "  for i in range(3):\n",
        "    count[i] = 1/count[i]\n",
        "  weights = []\n",
        "  for i in range(len(train_y)):\n",
        "    weights.append(count[train_y[i]])\n",
        "  weights = np.asarray(weights)\n",
        "  return weights\n",
        "\n",
        "def get_trained_LSTM_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length):\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  weights = get_weights(train_y)\n",
        "  model1 = Sequential()\n",
        "  e = Embedding(word_count+2, 50, weights = [embedding_matrix], input_length=max_sentence_length)\n",
        "  e.trainable = False\n",
        "  model1.add(e)\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Bidirectional(LSTM(100,activation=\"tanh\",recurrent_dropout=0.5,recurrent_activation=\"tanh\")))\n",
        "  model1.add(Flatten())#using all lstm hidden state ouputs\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Dense(50,activation=\"relu\",kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Dense(3,activation=\"softmax\"))\n",
        "\n",
        "  model1.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model1.fit(train_x,train_y,epochs=1)\n",
        "  e.trainable = True\n",
        "  model1.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model1.fit(train_x,train_y,epochs=4)\n",
        "  return model1\n",
        "  # predictions = model1.predict(test_x)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT-_UPR6gNv_"
      },
      "source": [
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Softmax, Bidirectional, Flatten, Average, GRU\n",
        "from keras.models import Sequential,Model\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_weights(train_y):\n",
        "  count = np.zeros(3,dtype = float)\n",
        "  for i in range(len(train_y)):\n",
        "    count[train_y[i]] += 1\n",
        "  for i in range(3):\n",
        "    count[i] = 1/count[i]\n",
        "  weights = []\n",
        "  for i in range(len(train_y)):\n",
        "    weights.append(count[train_y[i]])\n",
        "  weights = np.asarray(weights)\n",
        "  return weights\n",
        "\n",
        "def get_trained_GRU_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length):\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  weights = get_weights(train_y)\n",
        "  model1 = Sequential()\n",
        "  e = Embedding(word_count+2, 50, weights = [embedding_matrix], input_length=max_sentence_length)\n",
        "  e.trainable = False\n",
        "  model1.add(e)\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Bidirectional(GRU(100,activation=\"tanh\",recurrent_dropout=0.5,recurrent_activation=\"tanh\")))\n",
        "  model1.add(Flatten())#using all lstm hidden state ouputs\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Dense(50,activation=\"relu\",kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Dense(3,activation=\"softmax\"))\n",
        "\n",
        "  model1.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model1.fit(train_x,train_y,epochs=1)\n",
        "  e.trainable = True\n",
        "  model1.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model1.fit(train_x,train_y,epochs=4)\n",
        "  return model1\n",
        "  # predictions = model1.predict(test_x)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2yNTuEKB3Yn"
      },
      "source": [
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Softmax, Bidirectional, Flatten, Average, SimpleRNN\n",
        "from keras.models import Sequential,Model\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_RNN_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length):\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  model1 = Sequential()\n",
        "  e = Embedding(word_count+2, 50, weights = [embedding_matrix], input_length=max_sentence_length)\n",
        "  e.trainable = False\n",
        "  model1.add(e)\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Bidirectional(SimpleRNN(100,activation=\"tanh\",recurrent_dropout=0.5)))\n",
        "  model1.add(Flatten())#using all lstm hidden state ouputs\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Dense(50,activation=\"relu\",kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Dense(3,activation=\"softmax\"))\n",
        "\n",
        "  model1.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model1.fit(train_x,train_y,epochs=2)\n",
        "  #e.trainable = True\n",
        "  model1.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model1.fit(train_x,train_y,epochs=4)\n",
        "  return model1\n",
        "  # predictions = model1.predict(test_x)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM3fhH75Hgv1"
      },
      "source": [
        "from keras.layers import Lambda\n",
        "import keras.backend as K\n",
        "\n",
        "def get_weights(train_y):\n",
        "  count = np.zeros(3,dtype = float)\n",
        "  for i in range(len(train_y)):\n",
        "    count[train_y[i]] += 1\n",
        "  for i in range(5):\n",
        "    count[i] = len(train_y)/count[i]\n",
        "  weights = []\n",
        "  for i in range(len(train_y)):\n",
        "    weights.append(count[train_y[i]])\n",
        "  weights = np.asarray(weights)\n",
        "  return weights\n",
        "\n",
        "def get_modified_LSTM_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length,dev_x,dev_y):\n",
        "  #weights = get_weights(train_y)\n",
        "  #print(weights)\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  model2 = Sequential()\n",
        "  e = Embedding(word_count+2, 50, weights = [embedding_matrix], input_length=max_sentence_length)\n",
        "  model2.add(e)\n",
        "  model2.add(Dropout(0.5))\n",
        "  model2.add(Bidirectional(LSTM(100,activation=\"tanh\",recurrent_dropout=0.5,recurrent_activation=\"tanh\",return_sequences=True)))\n",
        "  model2.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(200,)))#using all lstm hidden state ouputs\n",
        "  model2.add(Dropout(0.5))\n",
        "  model2.add(Dense(50,activation=\"relu\"))\n",
        "  model2.add(Dropout(0.5))\n",
        "  model2.add(Dense(3,activation=\"softmax\"))\n",
        "\n",
        "  model2.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model2.fit(train_x,train_y,epochs=4,validation_data=(dev_x,dev_y))\n",
        "  # model2.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  # model2.fit(train_x,train_y,epochs=4)\n",
        "  return model2"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzvAFo7xIlmG"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(Attention,self).__init__()\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
        "                               initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
        "                               initializer=\"zeros\")\n",
        "        \n",
        "        super(Attention,self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x*a\n",
        "        \n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "        \n",
        "        return K.sum(output, axis=1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCYZ4Nc1IiL4"
      },
      "source": [
        "from keras.layers import Lambda\n",
        "import keras.backend as K\n",
        "\n",
        "def get_weights(train_y):\n",
        "  count = np.zeros(3,dtype = float)\n",
        "  for i in range(len(train_y)):\n",
        "    count[train_y[i]] += 1\n",
        "  for i in range(3):\n",
        "    count[i] = 1/count[i]\n",
        "  weights = []\n",
        "  for i in range(len(train_y)):\n",
        "    weights.append(count[train_y[i]])\n",
        "  weights = np.asarray(weights)\n",
        "  return weights\n",
        "\n",
        "def get_LSTM_Attn_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length,dev_x,dev_y):\n",
        "  weights = get_weights(train_y)\n",
        "  #print(weights)\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  model2 = Sequential()\n",
        "  e = Embedding(word_count+2, 50, weights = [embedding_matrix], input_length=max_sentence_length)\n",
        "  model2.add(e)\n",
        "  model2.add(Dropout(0.5))\n",
        "  model2.add(Bidirectional(LSTM(100,activation=\"tanh\",recurrent_dropout=0.5,recurrent_activation=\"tanh\",return_sequences=True)))\n",
        "  model2.add(Attention(return_sequences=False))#using all lstm hidden state ouputs\n",
        "  model2.add(Dropout(0.5))\n",
        "  model2.add(Dense(50,activation=\"relu\"))\n",
        "  model2.add(Dropout(0.5))\n",
        "  model2.add(Dense(3,activation=\"softmax\"))\n",
        "\n",
        "  model2.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'],loss_weights=weights)\n",
        "  model2.fit(train_x,train_y,epochs=4,validation_data=(dev_x,dev_y))\n",
        "  # model2.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  # model2.fit(train_x,train_y,epochs=4)\n",
        "  return model2"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjfqwoAAGCuI"
      },
      "source": [
        "# CNN model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Concatenate, Reshape\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_weights(train_y):\n",
        "  print(\"getting weights\")\n",
        "  count = np.zeros(5,dtype = float)\n",
        "  for i in range(len(train_y)):\n",
        "    count[train_y[i]] += 1\n",
        "  for i in range(5):\n",
        "    count[i] = len(train_y)/count[i]\n",
        "  weights = []\n",
        "  for i in range(len(train_y)):\n",
        "    weights.append(count[train_y[i]])\n",
        "  weights = np.asarray(weights)\n",
        "  return weights\n",
        "\n",
        "def get_trained_CNN_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length):\n",
        "  weights = get_weights(train_y)\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  input = Input(shape=(max_sentence_length,))\n",
        "  e = Embedding(word_count+2, 50, weights = [embedding_matrix], input_length=max_sentence_length)(input)\n",
        "  e.trainable = False\n",
        "  reshaped_embedding = Reshape((1,max_sentence_length,50,1))(e)\n",
        "  filter_2 = Conv2D(50,(2,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding)\n",
        "  filter_3 = Conv2D(50,(3,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding) \n",
        "  filter_4 = Conv2D(50,(4,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding)\n",
        "  reshaped_filter_2 = Reshape((max_sentence_length-1,50,1))(filter_2)\n",
        "  reshaped_filter_3 = Reshape((max_sentence_length-2,50,1))(filter_3)\n",
        "  reshaped_filter_4 = Reshape((max_sentence_length-3,50,1))(filter_4)\n",
        "  max_pooling_2 = MaxPooling2D(pool_size=(max_sentence_length-1,1))(reshaped_filter_2)\n",
        "  max_pooling_3 = MaxPooling2D(pool_size=(max_sentence_length-2,1))(reshaped_filter_3)\n",
        "  max_pooling_4 = MaxPooling2D(pool_size=(max_sentence_length-3,1))(reshaped_filter_4)\n",
        "  concatenated_output = Concatenate()([max_pooling_2,max_pooling_3,max_pooling_4])\n",
        "  concatenated_output_reshaped = Reshape((1,150))(concatenated_output)\n",
        "  dense1_output = Dense(50,activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(concatenated_output_reshaped)\n",
        "  dense2_output = Dense(3,activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(dense1_output)\n",
        "  softmax_probs = Softmax()(dense2_output)\n",
        "\n",
        "  model3 = Model(input,softmax_probs)\n",
        "  print(model3.summary())\n",
        "  print(\"compiling model\")\n",
        "  model3.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  print(\"fitting model\")\n",
        "  model3.fit(train_x,train_y,epochs=2)\n",
        "  e.trainable = True\n",
        "  model3.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model3.fit(train_x,train_y,epochs=4)\n",
        "  return model3"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBmYdR9MPyGi"
      },
      "source": [
        "#Converting the softmax outputs to their respective classes \n",
        "def get_predictions_single_model(model,test_x):\n",
        "  predictions = model.predict(test_x)\n",
        "  predictions = np.reshape(predictions,(len(test_x),3))\n",
        "  print(predictions)\n",
        "  predict_classes = []\n",
        "  for i in range(len(predictions)):\n",
        "    for j in range(3):\n",
        "      if predictions[i][j] == max(predictions[i]):\n",
        "        predict_classes.append(j)\n",
        "        break\n",
        "  predictions = predict_classes\n",
        "  print(predictions)\n",
        "  return predictions"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9h0oX5DqlTd"
      },
      "source": [
        "# MACRO-AVG-RECALL-METRIC\n",
        "def calculate_avg_recall(predictions,test_y):\n",
        "  true_pos_predcited,total_true_pos = 0,0\n",
        "  true_neg_predcited,total_true_neg = 0,0\n",
        "  true_neu_predcited,total_true_neu = 0,0\n",
        "  for i in range(len(predictions)):\n",
        "    if test_y[i] == 2:\n",
        "      total_true_pos += 1\n",
        "      if predictions[i] == 2:\n",
        "        true_pos_predcited += 1\n",
        "    elif test_y[i] == 0:\n",
        "      total_true_neg += 1\n",
        "      if predictions[i] == 0:\n",
        "        true_neg_predcited += 1\n",
        "    elif test_y[i] == 1:\n",
        "      total_true_neu += 1\n",
        "      if predictions[i] == 1:\n",
        "        true_neu_predcited += 1\n",
        "  print(true_pos_predcited/total_true_pos,true_neg_predcited/total_true_neg,true_neu_predcited/total_true_neu)\n",
        "  print(sum([true_pos_predcited/total_true_pos,true_neg_predcited/total_true_neg,true_neu_predcited/total_true_neu])/3)\n",
        "  return sum([true_pos_predcited/total_true_pos,true_neg_predcited/total_true_neg,true_neu_predcited/total_true_neu])/3"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWxow_lTnD06",
        "outputId": "4c5fd5fe-5068-4a9e-9e92-f91a672580b2"
      },
      "source": [
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "import gensim.downloader\n",
        "#model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/NLP/GoogleNews_Vectors_300.bin', binary=True)\n",
        "model = gensim.downloader.load('glove-twitter-50')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StUNnHUkrFmA"
      },
      "source": [
        "#Read the input file\n",
        "train_file = open(\"/content/drive/MyDrive/Datasets/train_2016\",'r',encoding='utf-8')\n",
        "text = train_file.read()\n",
        "train_file.close()\n",
        "\n",
        "text = replace_emojis(text)\n",
        "word_sentences = split_into_words(text)\n",
        "text = None\n",
        "word_sentences = remove_stopwords(word_sentences)\n",
        "print(word_sentences[0:2])\n",
        "sentences,sentence_classes,sentence_ids = seperate_classes_ids_sentences(word_sentences)\n",
        "sentences = remove_punctuations(sentences)\n",
        "word_sentences = None\n",
        "sentences = remove_recurring_characters(sentences)\n",
        "print(\"Creating sentence_vectors\")\n",
        "sentence_vectors,word_count,max_sentence_length,words_to_integer_dict = get_train_sentence_vectors(sentences,model)\n",
        "sentences = None\n",
        "\n",
        "#max_sentence_length = 512\n",
        "\n",
        "print(\"Word count,Max Sentence Length = \",word_count,max_sentence_length)\n",
        "\n",
        "data_x = pad_input_vectors(sentence_vectors,max_sentence_length)\n",
        "\n",
        "#train_file = open(\"/content/drive/My Drive/Colab Notebooks/NLP/Assignment-3/digital_music_5.y.train.txt\",'r',encoding='utf-8')\n",
        "#output_labels = train_file.readlines()\n",
        "#train_file.close()\n",
        "data_y = get_input_class_labels(sentence_classes)\n",
        "sentence_classes = None\n",
        "\n",
        "train_x,dev_x,train_y,dev_y = split_train_data(data_x,data_y)\n",
        "#data_x,data_y = None,None\n",
        "\n",
        "print(\"Creating Embeddings\")\n",
        "\n",
        "embedding_matrix = get_initial_embeddings(model,word_count,words_to_integer_dict)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjgnMV18HgcW"
      },
      "source": [
        "LSTM_model1 = get_trained_LSTM_model(data_x,data_y,embedding_matrix,word_count,max_sentence_length)\n",
        "predictions_LSTM_model1 = get_predictions_single_model(LSTM_model1,dev_x)\n",
        "LSTM1_avg_recall = calculate_avg_recall(predictions_LSTM_model1,dev_y)\n",
        "\n",
        "print(LSTM1_avg_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkiGSs6FTTVj"
      },
      "source": [
        "GRU_model = get_trained_GRU_model(data_x,data_y,embedding_matrix,word_count,max_sentence_length)\n",
        "predictions_GRU_model = get_predictions_single_model(GRU_model,dev_x)\n",
        "GRU_avg_recall = calculate_avg_recall(predictions_GRU_model,dev_y)\n",
        "\n",
        "print(GRU_avg_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlnkh44dCMSS"
      },
      "source": [
        "RNN_model1 = get_RNN_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length)\n",
        "predictions_RNN_model1 = get_predictions_single_model(RNN_model1,dev_x)\n",
        "RNN1_avg_recall = calculate_avg_recall(predictions_RNN_model1,dev_y)\n",
        "\n",
        "print(RNN1_avg_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TWKA3anIHw9"
      },
      "source": [
        "LSTM_avg_model = get_modified_LSTM_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length,dev_x,dev_y)\n",
        "predictions_LSTM_model2 = get_predictions_single_model(LSTM_avg_model,dev_x)\n",
        "LSTM2_avg_recall = calculate_avg_recall(predictions_LSTM_model2,dev_y)\n",
        "\n",
        "print(LSTM2_avg_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y568kzcKi0Pg"
      },
      "source": [
        "LSTM_model2 = get_LSTM_Attn_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length,dev_x,dev_y)\n",
        "predictions_LSTM_model2 = get_predictions_single_model(LSTM_model2,dev_x)\n",
        "LSTM2_avg_recall = calculate_avg_recall(predictions_LSTM_model2,dev_y)\n",
        "\n",
        "print(LSTM2_avg_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X_J4b8iIPX5"
      },
      "source": [
        "CNN_model = get_trained_CNN_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length)\n",
        "predictions_CNN_model = get_predictions_single_model(CNN_model,dev_x)\n",
        "CNN_avg_recall = calculate_avg_recall(predictions_CNN_model,dev_y)\n",
        "\n",
        "print(CNN_avg_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HfwNnQJtwFJ"
      },
      "source": [
        "test_file = open(\"/content/drive/MyDrive/Datasets/test_2016\",'r',encoding='utf-8')\n",
        "text = test_file.read()\n",
        "test_file.close()\n",
        "\n",
        "text = replace_emojis(text)\n",
        "word_sentences = split_into_words(text)\n",
        "sentences,sentence_classes,sentence_ids = seperate_classes_ids_sentences(word_sentences)\n",
        "sentences = remove_punctuations(sentences)\n",
        "sentences = remove_recurring_characters(sentences)\n",
        "sentence_vectors = get_test_sentence_vectors(sentences,word_count)\n",
        "test_x = pad_input_vectors(sentence_vectors,max_sentence_length)\n",
        "test_y = get_test_class_labels(sentence_classes)\n",
        "\n",
        "predictions_LSTM_model1 = get_predictions_single_model(GRU_model,test_x)\n",
        "LSTM1_avg_recall = calculate_avg_recall(predictions_LSTM_model1,test_y)\n",
        "\n",
        "predictions_LSTM_model2 = get_predictions_single_model(LSTM_avg_model,test_x)\n",
        "LSTM2_avg_recall = calculate_avg_recall(predictions_LSTM_model2,test_y)\n",
        "\n",
        "predictions_LSTM_Attn_model = get_predictions_single_model(LSTM_model2,test_x)\n",
        "LSTM2_avg_recall = calculate_avg_recall(predictions_LSTM_Attn_model ,test_y)\n",
        "\n",
        "predictions_CNN_model = get_predictions_single_model(CNN_model,test_x)\n",
        "CNN_avg_recall = calculate_avg_recall(predictions_CNN_model,test_y)\n",
        "\n",
        "#print(LSTM1_avg_recall,LSTM2_avg_recall,CNN_avg_recall)\n",
        "print(LSTM1_avg_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC3GHPd1LD2u"
      },
      "source": [
        "predictions_RNN_model1 = get_predictions_single_model(RNN_model1,dev_x)\n",
        "RNN1_avg_recall = calculate_avg_recall(predictions_RNN_model1,dev_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDrCpLy8RfaE"
      },
      "source": [
        "import pandas as pd\n",
        "output_df = pd.DataFrame({'predictions':predictions_LSTM_model1},index=None)\n",
        "output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/NLP/Project/Results/GRU_2016.csv',index=False) "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHZFre48ECaF"
      },
      "source": [
        "def parse_df_to_array(df):\n",
        "  predictions = []\n",
        "  #For true labels\n",
        "  for i in df['labels']:\n",
        "    predictions.append(int(i))\n",
        "  #For predictions\n",
        "  #for i in df['predictions']:\n",
        "    #predictions.append(int(i))\n",
        "  return predictions"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7L1odboD-O0"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/Results/GRU_2016.csv\")\n",
        "predictions_BERT_model = parse_df_to_array(df)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/Results/CNN_2016.csv\")\n",
        "predictions_CNN_model = parse_df_to_array(df)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/Results/LSTM_AVG_no_distant_training_2016.csv\")\n",
        "predictions_LSTM_AVG_model = parse_df_to_array(df)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/Results/LSTM_Attn_2016_weighted.csv\")\n",
        "predictions_LSTM_AVG_DT_model = parse_df_to_array(df)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/Results/LSTM_std_2016.csv\")\n",
        "predictions_LSTM_std_model = parse_df_to_array(df)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Effr9WhSG0mD",
        "outputId": "769a42ff-12ac-4055-f217-dc6c2b5a9ea0"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/Results/test_labels_2016.csv\")\n",
        "#print(df.head)\n",
        "true_labels = parse_df_to_array(df)\n",
        "#print(true_labels[1648])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-VH6ppZj2jb"
      },
      "source": [
        "ensemble_predictions = []\n",
        "for i in range(len(true_labels)):\n",
        "  votes = [0]*3\n",
        "  votes[predictions_BERT_model[i]] += 1\n",
        "  votes[predictions_CNN_model[i]] += 1\n",
        "  votes[predictions_LSTM_AVG_model[i]] += 1\n",
        "  votes[predictions_LSTM_AVG_DT_model[i]] += 1\n",
        "  votes[predictions_LSTM_std_model[i]] += 1\n",
        "  for j in range(3):\n",
        "    if votes[j] == max(votes):\n",
        "      ensemble_predictions.append(j)\n",
        "      break\n",
        "ensemble_avg_recall = calculate_avg_recall(ensemble_predictions,true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
